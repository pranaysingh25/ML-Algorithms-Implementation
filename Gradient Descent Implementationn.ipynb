{"cells":[{"metadata":{},"cell_type":"markdown","source":"*     importing essential libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport sys","execution_count":55,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* importing the BOSTON Dataset from sklearn"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = boston.data\ny = boston.target","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's convert the data into pandas DataFrame and explore it a littleÂ bit"},{"metadata":{"trusted":true},"cell_type":"code","source":"boston = pd.DataFrame(boston.data, columns = boston.feature_names)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.head()","execution_count":59,"outputs":[{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  \n0     15.3  396.90   4.98  \n1     17.8  396.90   9.14  \n2     17.8  392.83   4.03  \n3     18.7  394.63   2.94  \n4     18.7  396.90   5.33  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.describe()","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\ncount  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \nmean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \nstd      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \nmin      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \nmax     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n\n              AGE         DIS         RAD         TAX     PTRATIO           B  \\\ncount  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \nmean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \nstd     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \nmin      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \nmax    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n\n            LSTAT  \ncount  506.000000  \nmean    12.653063  \nstd      7.141062  \nmin      1.730000  \n25%      6.950000  \n50%     11.360000  \n75%     16.955000  \nmax     37.970000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.613524</td>\n      <td>11.363636</td>\n      <td>11.136779</td>\n      <td>0.069170</td>\n      <td>0.554695</td>\n      <td>6.284634</td>\n      <td>68.574901</td>\n      <td>3.795043</td>\n      <td>9.549407</td>\n      <td>408.237154</td>\n      <td>18.455534</td>\n      <td>356.674032</td>\n      <td>12.653063</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.601545</td>\n      <td>23.322453</td>\n      <td>6.860353</td>\n      <td>0.253994</td>\n      <td>0.115878</td>\n      <td>0.702617</td>\n      <td>28.148861</td>\n      <td>2.105710</td>\n      <td>8.707259</td>\n      <td>168.537116</td>\n      <td>2.164946</td>\n      <td>91.294864</td>\n      <td>7.141062</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.006320</td>\n      <td>0.000000</td>\n      <td>0.460000</td>\n      <td>0.000000</td>\n      <td>0.385000</td>\n      <td>3.561000</td>\n      <td>2.900000</td>\n      <td>1.129600</td>\n      <td>1.000000</td>\n      <td>187.000000</td>\n      <td>12.600000</td>\n      <td>0.320000</td>\n      <td>1.730000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.082045</td>\n      <td>0.000000</td>\n      <td>5.190000</td>\n      <td>0.000000</td>\n      <td>0.449000</td>\n      <td>5.885500</td>\n      <td>45.025000</td>\n      <td>2.100175</td>\n      <td>4.000000</td>\n      <td>279.000000</td>\n      <td>17.400000</td>\n      <td>375.377500</td>\n      <td>6.950000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.256510</td>\n      <td>0.000000</td>\n      <td>9.690000</td>\n      <td>0.000000</td>\n      <td>0.538000</td>\n      <td>6.208500</td>\n      <td>77.500000</td>\n      <td>3.207450</td>\n      <td>5.000000</td>\n      <td>330.000000</td>\n      <td>19.050000</td>\n      <td>391.440000</td>\n      <td>11.360000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.677083</td>\n      <td>12.500000</td>\n      <td>18.100000</td>\n      <td>0.000000</td>\n      <td>0.624000</td>\n      <td>6.623500</td>\n      <td>94.075000</td>\n      <td>5.188425</td>\n      <td>24.000000</td>\n      <td>666.000000</td>\n      <td>20.200000</td>\n      <td>396.225000</td>\n      <td>16.955000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>88.976200</td>\n      <td>100.000000</td>\n      <td>27.740000</td>\n      <td>1.000000</td>\n      <td>0.871000</td>\n      <td>8.780000</td>\n      <td>100.000000</td>\n      <td>12.126500</td>\n      <td>24.000000</td>\n      <td>711.000000</td>\n      <td>22.000000</td>\n      <td>396.900000</td>\n      <td>37.970000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"* **We can clearly see the irregular scale of data, since we are going to implement Gradient Descent which when scale is improper takes long time to converge to the optimal value of parameters, So we need to scale the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nScale = StandardScaler()\nboston = Scale.fit_transform(boston)","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston = pd.DataFrame(boston, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston['bias']=1","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.head()","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n0 -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n1 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n2 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n3 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n4 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n\n        DIS       RAD       TAX   PTRATIO         B     LSTAT  bias  \n0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562     1  \n1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439     1  \n2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727     1  \n3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517     1  \n4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501     1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>bias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.419782</td>\n      <td>0.284830</td>\n      <td>-1.287909</td>\n      <td>-0.272599</td>\n      <td>-0.144217</td>\n      <td>0.413672</td>\n      <td>-0.120013</td>\n      <td>0.140214</td>\n      <td>-0.982843</td>\n      <td>-0.666608</td>\n      <td>-1.459000</td>\n      <td>0.441052</td>\n      <td>-1.075562</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.417339</td>\n      <td>-0.487722</td>\n      <td>-0.593381</td>\n      <td>-0.272599</td>\n      <td>-0.740262</td>\n      <td>0.194274</td>\n      <td>0.367166</td>\n      <td>0.557160</td>\n      <td>-0.867883</td>\n      <td>-0.987329</td>\n      <td>-0.303094</td>\n      <td>0.441052</td>\n      <td>-0.492439</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.417342</td>\n      <td>-0.487722</td>\n      <td>-0.593381</td>\n      <td>-0.272599</td>\n      <td>-0.740262</td>\n      <td>1.282714</td>\n      <td>-0.265812</td>\n      <td>0.557160</td>\n      <td>-0.867883</td>\n      <td>-0.987329</td>\n      <td>-0.303094</td>\n      <td>0.396427</td>\n      <td>-1.208727</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.416750</td>\n      <td>-0.487722</td>\n      <td>-1.306878</td>\n      <td>-0.272599</td>\n      <td>-0.835284</td>\n      <td>1.016303</td>\n      <td>-0.809889</td>\n      <td>1.077737</td>\n      <td>-0.752922</td>\n      <td>-1.106115</td>\n      <td>0.113032</td>\n      <td>0.416163</td>\n      <td>-1.361517</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.412482</td>\n      <td>-0.487722</td>\n      <td>-1.306878</td>\n      <td>-0.272599</td>\n      <td>-0.835284</td>\n      <td>1.228577</td>\n      <td>-0.511180</td>\n      <td>1.077737</td>\n      <td>-0.752922</td>\n      <td>-1.106115</td>\n      <td>0.113032</td>\n      <td>0.441052</td>\n      <td>-1.026501</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the data into training data and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test , y_train, y_test = train_test_split(boston, y, test_size = 0.2)","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we're talking! We've done little bit of required preprocessing, lets write down the code that will use Gradient Descent that will optimize the hyperparameter in order to minimise the cost function\n\n* We will first write code for 1 step of gradient descent working, initializing the parameters randomly, calculating gradients using them and then updating the value of parameters by substracting the gradient from them"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(X_train)\nX_test=np.array(X_test)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def step_gradient(X, y, learning_rate, theta):\n    k = X.shape[0]\n    n = X.shape[1]\n    gradients = np.zeros(n)\n    for i in range(k):\n        for j in range(n):\n            gradients[j] += (-2/k) * ( y[i] - (theta.dot(X[i,:])) ) * X[i,j]\n    theta = theta - learning_rate * gradients\n    return theta","execution_count":67,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now defining the Gradient Descent function that will call step gradient fuction for the defined no. of iterations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(X, y, learning_rate, iterations):\n    k = X.shape[0]\n    n = X.shape[1]\n    theta = np.zeros(n)                #random initialization\n    for i in range(iterations):\n#         gradient = 2/k * X.T.dot(X.dot(theta) - y)   (direct formula for gradient vector)\n#         theta = theta - learning_rate * gradient\n        theta = step_gradient(X, y, learning_rate, theta)\n        print(i, 'cost:', cost(X, y, theta))\n    return theta","execution_count":68,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It's pretty much done, let's now write down the main business - the cost function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost(X, y, theta):\n    k = X.shape[0]\n    total_cost = 0\n    for i in range(k):\n        total_cost += 1/k * (y[i] - (theta.dot(X[i,:])))**2\n    return total_cost","execution_count":69,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We're done now, except for the main command giving function 'run' , which will run our algorithm just taking the training data and its labels as input from us, to train itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(X, y):\n    learning_rate = 0.04\n    iterations = 300\n    theta = gradient_descent(X, y, learning_rate, iterations)\n    return theta","execution_count":70,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try this on our boston training data "},{"metadata":{"trusted":true},"cell_type":"code","source":"theta = run(X_train, y_train)\n","execution_count":71,"outputs":[{"output_type":"stream","text":"0 cost: 496.7183418700804\n1 cost: 417.727998422731\n2 cost: 354.89470562401516\n3 cost: 302.99013858431954\n4 cost: 259.54613779707796\n5 cost: 223.0167637696991\n6 cost: 192.24843097631629\n7 cost: 166.31217845183073\n8 cost: 144.43872750336786\n9 cost: 125.98479144830647\n10 cost: 110.41055213673926\n11 cost: 97.26237312522896\n12 cost: 86.15876392334638\n13 cost: 76.77875006555017\n14 cost: 68.85216674516168\n15 cost: 62.151535358901995\n16 cost: 56.485256101690844\n17 cost: 51.691898985516524\n18 cost: 47.63541321585998\n19 cost: 44.20110513244426\n20 cost: 41.29225981893488\n21 cost: 38.82730211587408\n22 cost: 36.737409916613764\n23 cost: 34.96450689823811\n24 cost: 33.45957373259702\n25 cost: 32.1812267418552\n26 cost: 31.094521242530895\n27 cost: 30.169943738026255\n28 cost: 29.382562900763123\n29 cost: 28.71131412071709\n30 cost: 28.138396444586373\n31 cost: 27.648764119594052\n32 cost: 27.22969779651822\n33 cost: 26.870442828259375\n34 cost: 26.561904098261493\n35 cost: 26.296388490063343\n36 cost: 26.067387517414296\n37 cost: 25.869393817369403\n38 cost: 25.697746203005458\n39 cost: 25.548498808333484\n40 cost: 25.41831056108953\n41 cost: 25.304351810684615\n42 cost: 25.204225436545592\n43 cost: 25.11590018133259\n44 cost: 25.037654306629108\n45 cost: 24.968027966185176\n46 cost: 24.90578294248254\n47 cost: 24.849868603702674\n48 cost: 24.79939311633966\n49 cost: 24.753599098947625\n50 cost: 24.711843029245994\n51 cost: 24.67357782373024\n52 cost: 24.638338099161047\n53 cost: 24.60572770145778\n54 cost: 24.575409151805196\n55 cost: 24.547094714056364\n56 cost: 24.520538833346336\n57 cost: 24.495531734537227\n58 cost: 24.47189400181063\n59 cost: 24.449471988343408\n60 cost: 24.42813392834019\n61 cost: 24.407766643415197\n62 cost: 24.388272751982377\n63 cost: 24.369568304398513\n64 cost: 24.351580778511412\n65 cost: 24.33424738033079\n66 cost: 24.317513603052344\n67 cost: 24.301332004859944\n68 cost: 24.285661172018607\n69 cost: 24.270464838917334\n70 cost: 24.255711141075015\n71 cost: 24.241371980805\n72 cost: 24.22742248834913\n73 cost: 24.213840563929875\n74 cost: 24.20060648839658\n75 cost: 24.187702592032174\n76 cost: 24.17511297268219\n77 cost: 24.162823255720127\n78 cost: 24.15082038950845\n79 cost: 24.139092470981527\n80 cost: 24.127628596799024\n81 cost: 24.116418736210296\n82 cost: 24.105453622360344\n83 cost: 24.094724659264813\n84 cost: 24.084223842103107\n85 cost: 24.073943688836753\n86 cost: 24.063877181461645\n87 cost: 24.05401771546017\n88 cost: 24.044359056236193\n89 cost: 24.03489530149939\n90 cost: 24.025620848722273\n91 cost: 24.01653036692526\n92 cost: 24.007618772156775\n93 cost: 23.998881206131504\n94 cost: 23.990313017569445\n95 cost: 23.981909745847496\n96 cost: 23.97366710663325\n97 cost: 23.965580979219688\n98 cost: 23.957647395321626\n99 cost: 23.94986252912965\n100 cost: 23.942222688448112\n101 cost: 23.93472430676889\n102 cost: 23.927363936154503\n103 cost: 23.920138240822464\n104 cost: 23.913043991338974\n105 cost: 23.9060780593425\n106 cost: 23.899237412730386\n107 cost: 23.89251911124979\n108 cost: 23.885920302444372\n109 cost: 23.879438217912824\n110 cost: 23.87307016984367\n111 cost: 23.8668135477943\n112 cost: 23.860665815686907\n113 cost: 23.854624508998114\n114 cost: 23.848687232121744\n115 cost: 23.842851655887213\n116 cost: 23.83711551521824\n117 cost: 23.831476606918244\n118 cost: 23.82593278757143\n119 cost: 23.820481971548666\n120 cost: 23.815122129109835\n121 cost: 23.80985128459446\n122 cost: 23.80466751469383\n123 cost: 23.799568946798296\n124 cost: 23.79455375741489\n125 cost: 23.789620170649794\n126 cost: 23.784766456751594\n127 cost: 23.779990930711598\n128 cost: 23.775291950917488\n129 cost: 23.770667917857597\n130 cost: 23.76611727287221\n131 cost: 23.761638496950297\n132 cost: 23.757230109568496\n133 cost: 23.752890667570725\n134 cost: 23.748618764086018\n135 cost: 23.744413027483386\n136 cost: 23.74027212036101\n137 cost: 23.73619473856981\n138 cost: 23.73217961026791\n139 cost: 23.728225495006374\n140 cost: 23.7243311828443\n141 cost: 23.72049549349162\n142 cost: 23.71671727547948\n143 cost: 23.712995405356363\n144 cost: 23.709328786909243\n145 cost: 23.705716350409016\n146 cost: 23.702157051878654\n147 cost: 23.69864987238463\n148 cost: 23.695193817348922\n149 cost: 23.691787915882557\n150 cost: 23.68843122013883\n151 cost: 23.68512280468618\n152 cost: 23.681861765899562\n153 cost: 23.678647221370333\n154 cost: 23.675478309333187\n155 cost: 23.67235418811028\n156 cost: 23.66927403557166\n157 cost: 23.666237048611578\n158 cost: 23.66324244264023\n159 cost: 23.660289451089866\n160 cost: 23.65737732493591\n161 cost: 23.654505332231643\n162 cost: 23.651672757656133\n163 cost: 23.648878902075822\n164 cost: 23.64612308211808\n165 cost: 23.64340462975747\n166 cost: 23.640722891913704\n167 cost: 23.63807723006098\n168 cost: 23.635467019848676\n169 cost: 23.632891650732837\n170 cost: 23.63035052561776\n171 cost: 23.627843060508404\n172 cost: 23.625368684171896\n173 cost: 23.62292683780914\n174 cost: 23.620516974735505\n175 cost: 23.618138560070413\n176 cost: 23.615791070435655\n177 cost: 23.61347399366232\n178 cost: 23.61118682850576\n179 cost: 23.608929084368494\n180 cost: 23.606700281031035\n181 cost: 23.60449994838996\n182 cost: 23.60232762620333\n183 cost: 23.600182863843393\n184 cost: 23.598065220055652\n185 cost: 23.59597426272502\n186 cost: 23.593909568648318\n187 cost: 23.591870723312816\n188 cost: 23.58985732068112\n189 cost: 23.587868962981922\n190 cost: 23.585905260506284\n191 cost: 23.58396583140971\n192 cost: 23.582050301519498\n193 cost: 23.58015830414741\n194 cost: 23.57828947990736\n195 cost: 23.576443476538117\n196 cost: 23.574619948730707\n197 cost: 23.572818557960765\n198 cost: 23.57103897232503\n199 cost: 23.56928086638239\n200 cost: 23.567543920999462\n201 cost: 23.56582782319992\n202 cost: 23.564132266018184\n203 cost: 23.56245694835661\n204 cost: 23.56080157484727\n205 cost: 23.559165855716433\n206 cost: 23.557549506653487\n207 cost: 23.555952248682818\n208 cost: 23.5543738080393\n209 cost: 23.55281391604717\n210 cost: 23.55127230900194\n211 cost: 23.549748728055512\n212 cost: 23.548242919104087\n213 cost: 23.546754632679555\n214 cost: 23.545283623843222\n215 cost: 23.543829652082458\n216 cost: 23.54239248121024\n217 cost: 23.540971879267083\n218 cost: 23.53956761842563\n219 cost: 23.538179474897646\n220 cost: 23.53680722884361\n221 cost: 23.53545066428448\n222 cost: 23.534109569015712\n223 cost: 23.532783734523615\n224 cost: 23.531472955903855\n225 cost: 23.530177031781882\n226 cost: 23.528895764235685\n227 cost: 23.527628958720285\n228 cost: 23.526376423994208\n229 cost: 23.52513797204775\n230 cost: 23.523913418033533\n231 cost: 23.522702580197972\n232 cost: 23.521505279815358\n233 cost: 23.52032134112302\n234 cost: 23.519150591258263\n235 cost: 23.517992860197115\n236 cost: 23.516847980694365\n237 cost: 23.515715788225\n238 cost: 23.514596120927557\n239 cost: 23.513488819548407\n240 cost: 23.512393727387558\n241 cost: 23.51131069024618\n242 cost: 23.51023955637473\n243 cost: 23.509180176423\n244 cost: 23.508132403390956\n245 cost: 23.507096092581143\n246 cost: 23.506071101551793\n247 cost: 23.5050572900717\n248 cost: 23.504054520075464\n249 cost: 23.503062655620575\n250 cost: 23.502081562844868\n251 cost: 23.501111109925564\n252 cost: 23.50015116703871\n253 cost: 23.499201606320216\n254 cost: 23.49826230182749\n255 cost: 23.497333129501843\n256 cost: 23.496413967132096\n257 cost: 23.49550469431907\n258 cost: 23.494605192440496\n259 cost: 23.493715344617243\n260 cost: 23.49283503568006\n261 cost: 23.49196415213717\n262 cost: 23.491102582142716\n263 cost: 23.49025021546574\n264 cost: 23.489406943459958\n265 cost: 23.488572659034496\n266 cost: 23.487747256624928\n267 cost: 23.48693063216502\n268 cost: 23.48612268305956\n269 cost: 23.485323308157113\n270 cost: 23.484532407724146\n271 cost: 23.48374988341906\n272 cost: 23.482975638267376\n273 cost: 23.48220957663698\n274 cost: 23.48145160421433\n275 cost: 23.480701627981112\n276 cost: 23.479959556191083\n277 cost: 23.47922529834802\n278 cost: 23.478498765183517\n279 cost: 23.47777986863583\n280 cost: 23.477068521828947\n281 cost: 23.476364639051777\n282 cost: 23.47566813573865\n283 cost: 23.47497892844937\n284 cost: 23.47429693485022\n285 cost: 23.473622073695054\n286 cost: 23.472954264807225\n287 cost: 23.472293429061434\n288 cost: 23.471639488366172\n289 cost: 23.47099236564679\n290 cost: 23.470351984828206\n291 cost: 23.469718270819136\n292 cost: 23.469091149495153\n293 cost: 23.46847054768364\n","name":"stdout"},{"output_type":"stream","text":"294 cost: 23.467856393147915\n295 cost: 23.467248614572334\n296 cost: 23.466647141547266\n297 cost: 23.4660519045548\n298 cost: 23.465462834954433\n299 cost: 23.464879864969184\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta","execution_count":72,"outputs":[{"output_type":"execute_result","execution_count":72,"data":{"text/plain":"array([-1.14412931,  0.95760734,  0.19832997,  0.92967289, -2.26076775,\n        2.72607617,  0.0602581 , -2.96523504,  2.55655277, -1.64377654,\n       -2.13322166,  0.95435653, -3.65686694, 22.6581299 ])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X, m):\n    N = X.shape[0]\n    Y = np.zeros(N)\n    for i in range(N):\n        Y[i] = (theta * X[i,:]).sum()\n    return Y","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = predict(X_test, theta)","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score(y_pred,y_test):\n    u = ((y_test - y_pred)**2).sum()\n    v = ((y_test - y_test.mean())**2).sum()\n    return 1 - u/v","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(y_pred, y_test)","execution_count":76,"outputs":[{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"0.7830207062531909"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}